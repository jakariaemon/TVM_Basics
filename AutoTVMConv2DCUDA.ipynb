{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03b_TVM_Tutorial_AutoTVMConv2DCUDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9soFF1q_cV-y"
      },
      "source": [
        "Please run the following block to ensure TVM is setup for *this notebook*, each notebook may have its own runtime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_lzOGuFcUgG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90390399-ef0c-448c-feb1-a8ec070fff07"
      },
      "source": [
        "! gsutil cp \"gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz\" /tmp/tvm.tar.gz\n",
        "! mkdir -p /tvm\n",
        "! tar -xf /tmp/tvm.tar.gz --strip-components=4 --directory /tvm\n",
        "! ls -la /tvm\n",
        "# Move this block after we are done with pkg step\n",
        "! bash /tvm/package.sh\n",
        "import sys\n",
        "sys.path.append('/tvm/python')\n",
        "sys.path.append('/tvm/topi/python')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz...\n",
            "- [1 files][119.5 MiB/119.5 MiB]                                                \n",
            "Operation completed over 1 objects/119.5 MiB.                                    \n",
            "total 164\n",
            "drwxr-xr-x 21 root root  4096 Sep 23 08:21 .\n",
            "drwxr-xr-x  1 root root  4096 Sep 23 08:21 ..\n",
            "drwx------  8 root root  4096 May 31  2019 3rdparty\n",
            "drwx------ 12 root root  4096 May 31  2019 apps\n",
            "drwx------  3 root root  4096 Jun 19  2019 build\n",
            "drwx------  4 root root  4096 May 31  2019 cmake\n",
            "-rw-------  1 root root 11053 Jun 19  2019 CMakeLists.txt\n",
            "drwx------  6 root root  4096 May 31  2019 conda\n",
            "-rw-------  1 root root  5736 Jun 19  2019 CONTRIBUTORS.md\n",
            "drwx------  3 root root  4096 May 31  2019 docker\n",
            "drwx------ 11 root root  4096 May 31  2019 docs\n",
            "drwx------  4 root root  4096 May 31  2019 golang\n",
            "drwx------  3 root root  4096 May 31  2019 include\n",
            "-rw-------  1 root root 10607 Jun 19  2019 Jenkinsfile\n",
            "drwx------  6 root root  4096 May 31  2019 jvm\n",
            "-rw-------  1 root root 11357 Jun 19  2019 LICENSE\n",
            "-rw-------  1 root root  4267 Jun 19  2019 Makefile\n",
            "-rw-------  1 root root 10476 Jun 19  2019 NEWS.md\n",
            "drwx------  9 root root  4096 May 31  2019 nnvm\n",
            "-rw-------  1 root root    61 Jun 19  2019 NOTICE\n",
            "-rwx------  1 root root   374 Jun 19  2019 package.sh\n",
            "drwx------  3 root root  4096 May 31  2019 python\n",
            "-rw-------  1 root root  2705 Jun 19  2019 README.md\n",
            "drwx------  6 root root  4096 May 31  2019 rust\n",
            "drwx------ 14 root root  4096 May 31  2019 src\n",
            "drwx------  9 root root  4096 May 31  2019 tests\n",
            "drwx------  7 root root  4096 May 31  2019 topi\n",
            "drwx------  8 root root  4096 May 31  2019 tutorials\n",
            "-rw-------  1 root root  2902 Jun 19  2019 version.py\n",
            "drwx------ 10 root root  4096 May 31  2019 vta\n",
            "drwx------  2 root root  4096 May 31  2019 web\n",
            "Installing Dependencies ...\n",
            "deb https://dl.bintray.com/sbt/debian /\n",
            "Executing: /tmp/apt-key-gpghome.Av3o4u0gWC/gpg.1.sh --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\n",
            "gpg: key 99E82A75642AC823: public key \"sbt build tool <scalasbt@gmail.com>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Err:5 https://dl.bintray.com/sbt/debian  InRelease\n",
            "  502  Bad Gateway [IP: 52.34.230.170 443]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Ign:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:11 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [92.1 kB]\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,161 kB]\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,108 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,202 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,545 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,990 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,322 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,080 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,422 kB]\n",
            "Fetched 16.2 MB in 5s (3,444 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Failed to fetch https://dl.bintray.com/sbt/debian/InRelease  502  Bad Gateway [IP: 52.34.230.170 443]\n",
            "W: Some index files failed to download. They have been ignored, or old ones used instead.\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "clinfo is already the newest version (2.2.18.03.26-1).\n",
            "libtinfo-dev is already the newest version (6.1-1ubuntu1.18.04).\n",
            "libtinfo-dev set to manually installed.\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2.2).\n",
            "zlib1g-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  llvm-6.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  binfmt-support libffi-dev llvm-6.0 llvm-6.0-dev llvm-6.0-runtime tree\n",
            "0 upgraded, 6 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 28.3 MB of archives.\n",
            "After this operation, 178 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 binfmt-support amd64 2.1.8-2 [51.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0-runtime amd64 1:6.0-1ubuntu2 [200 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0 amd64 1:6.0-1ubuntu2 [4,838 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libffi-dev amd64 3.2.1-8 [156 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0-dev amd64 1:6.0-1ubuntu2 [23.0 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 28.3 MB in 4s (6,423 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package binfmt-support.\n",
            "(Reading database ... 155569 files and directories currently installed.)\n",
            "Preparing to unpack .../0-binfmt-support_2.1.8-2_amd64.deb ...\n",
            "Unpacking binfmt-support (2.1.8-2) ...\n",
            "Selecting previously unselected package llvm-6.0-runtime.\n",
            "Preparing to unpack .../1-llvm-6.0-runtime_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0-runtime (1:6.0-1ubuntu2) ...\n",
            "Selecting previously unselected package llvm-6.0.\n",
            "Preparing to unpack .../2-llvm-6.0_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0 (1:6.0-1ubuntu2) ...\n",
            "Selecting previously unselected package libffi-dev:amd64.\n",
            "Preparing to unpack .../3-libffi-dev_3.2.1-8_amd64.deb ...\n",
            "Unpacking libffi-dev:amd64 (3.2.1-8) ...\n",
            "Selecting previously unselected package llvm-6.0-dev.\n",
            "Preparing to unpack .../4-llvm-6.0-dev_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0-dev (1:6.0-1ubuntu2) ...\n",
            "Selecting previously unselected package tree.\n",
            "Preparing to unpack .../5-tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up binfmt-support (2.1.8-2) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/binfmt-support.service â†’ /lib/systemd/system/binfmt-support.service.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Setting up libffi-dev:amd64 (3.2.1-8) ...\n",
            "Setting up llvm-6.0-runtime (1:6.0-1ubuntu2) ...\n",
            "Setting up llvm-6.0 (1:6.0-1ubuntu2) ...\n",
            "Setting up llvm-6.0-dev (1:6.0-1ubuntu2) ...\n",
            "Processing triggers for systemd (237-3ubuntu10.56) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package sbt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agr421w8a49k"
      },
      "source": [
        "Import packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9JnukA4aTLJ"
      },
      "source": [
        "import logging\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "import tvm\n",
        "import topi\n",
        "from topi.testing import conv2d_nchw_python\n",
        "\n",
        "from tvm import autotvm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5YlgPYma_5r"
      },
      "source": [
        "Step 0: Vanilla direct 2D convolution implementation without a tunable template\n",
        "---------------------------------------------------------------------------------------------\n",
        "\n",
        "We reuse the conv2d with NCHW data layout in the TVM operator inventory (TOPI).\n",
        "This definition gives us the default schedule (loop nest) seen below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhtW-j3bbMLI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a3fff0e-d0d4-457c-80c6-c398c91d8295"
      },
      "source": [
        "# the last layer in resnet\n",
        "N, H, W, CO, CI, KH, KW, stride, padding = 1, 7, 7, 512, 512, 3, 3, (1, 1), (1, 1)\n",
        "assert N == 1, \"Only consider batch_size = 1 in this template\"\n",
        "\n",
        "data = tvm.placeholder((N, CI, H, W), name='data')\n",
        "kernel = tvm.placeholder((CO, CI, KH, KW), name='kernel')\n",
        "conv = topi.nn.conv2d_nchw(data, kernel, stride, padding, dilation=1, out_dtype='float32')\n",
        "s = tvm.create_schedule([conv.op])\n",
        "print(\"Default Schedule:\")\n",
        "print(tvm.lower(s, [data, kernel, conv], simple_mode=True))\n",
        "\n",
        "# assign axes of the default schedule to variables\n",
        "n, f, y, x = s[conv].op.axis\n",
        "rc, ry, rx = s[conv].op.reduce_axis"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default Schedule:\n",
            "// attr [pad_temp] storage_scope = \"global\"\n",
            "allocate pad_temp[float32 * 41472]\n",
            "produce pad_temp {\n",
            "  for (i1, 0, 512) {\n",
            "    for (i2, 0, 9) {\n",
            "      for (i3, 0, 9) {\n",
            "        pad_temp[((((i1*9) + i2)*9) + i3)] = tvm_if_then_else(((((1 <= i2) && (i2 < 8)) && (1 <= i3)) && (i3 < 8)), data[(((((i1*7) + i2)*7) + i3) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  for (ff, 0, 512) {\n",
            "    for (yy, 0, 7) {\n",
            "      for (xx, 0, 7) {\n",
            "        compute[((((ff*7) + yy)*7) + xx)] = 0.000000f\n",
            "        for (rc, 0, 512) {\n",
            "          for (ry, 0, 3) {\n",
            "            for (rx, 0, 3) {\n",
            "              compute[((((ff*7) + yy)*7) + xx)] = (compute[((((ff*7) + yy)*7) + xx)] + (pad_temp[((((((rc*9) + yy) + ry)*9) + xx) + rx)]*kernel[((((((ff*512) + rc)*3) + ry)*3) + rx)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYacZkkNd_xG"
      },
      "source": [
        "Here, we inline padding into the computation (as opposed to padding in the input in a second operator) and declare cache stages. Cache stages are prepare a subset of the input (read) or output (write) for improved temporal locality with higher performance memories (e.g., registers and shared memory vs. global memory)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afD69lgUd-QG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b652de-cfb8-4ef0-e05e-a93cfb5072b7"
      },
      "source": [
        "# inline padding\n",
        "pad_data = s[conv].op.input_tensors[0]\n",
        "s[pad_data].compute_inline()\n",
        "input = data\n",
        "data, raw_data = pad_data, data\n",
        "\n",
        "output = conv\n",
        "OL = s.cache_write(conv, 'local')\n",
        "\n",
        "# create cache stage\n",
        "AA = s.cache_read(data, 'shared', [OL])\n",
        "WW = s.cache_read(kernel, 'shared', [OL])\n",
        "AL = s.cache_read(AA, 'local', [OL])\n",
        "WL = s.cache_read(WW, 'local', [OL])\n",
        "\n",
        "print(tvm.lower(s, [input, kernel, conv], simple_mode=True))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "// attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "allocate pad_temp.shared[float32 * 41472]\n",
            "// attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "allocate pad_temp.shared.local[float32 * 41472]\n",
            "// attr [kernel.shared] storage_scope = \"shared\"\n",
            "allocate kernel.shared[float32 * 2359296]\n",
            "// attr [kernel.shared.local] storage_scope = \"local\"\n",
            "allocate kernel.shared.local[float32 * 2359296]\n",
            "// attr [compute.local] storage_scope = \"local\"\n",
            "allocate compute.local[float32 * 25088]\n",
            "produce pad_temp.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)] = tvm_if_then_else(((((1 <= ax2) && (ax2 < 8)) && (1 <= ax3)) && (ax3 < 8)), data[(((((ax1*7) + ax2)*7) + ax3) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce pad_temp.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared.local[((((ax1*9) + ax2)*9) + ax3)] = pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared.local {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared.local[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute.local {\n",
            "  for (ff.c, 0, 512) {\n",
            "    for (yy.c, 0, 7) {\n",
            "      for (xx.c, 0, 7) {\n",
            "        compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = 0.000000f\n",
            "        for (rc, 0, 512) {\n",
            "          for (ry, 0, 3) {\n",
            "            for (rx, 0, 3) {\n",
            "              compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = (compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] + (pad_temp.shared.local[((((((rc*9) + yy.c) + ry)*9) + xx.c) + rx)]*kernel.shared.local[((((((ff.c*512) + rc)*3) + ry)*3) + rx)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  for (ff, 0, 512) {\n",
            "    for (yy, 0, 7) {\n",
            "      for (xx, 0, 7) {\n",
            "        compute[((((ff*7) + yy)*7) + xx)] = compute.local[((((ff*7) + yy)*7) + xx)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDGX2K6MeWH6"
      },
      "source": [
        "Here, we first grab the spatial axes from the schedule. Next, we define several magic numbers that are tiling factors that we use to split the original loop nest into one with several additional levels. We reorder the levels to redefine the computation order (and the memory access order) or the computation. As we will see in the next cell, this transformation also readies the schedule for a mapping from loop nests to GPU computation indicies (grids, blocks, threads)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLaKLAH7eWRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc9f2c3-73cf-45c7-d511-f45dd16033c7"
      },
      "source": [
        "# tile spatial axes\n",
        "n, f, y, x = s[output].op.axis\n",
        "tile_f_factors = [8, 8, 8, 1]\n",
        "tile_x_factors = [7, 7, 7, 1]\n",
        "tile_y_factors = [7, 7, 7, 1]\n",
        "\n",
        "bf, vf = s[output].split(f, factor=tile_f_factors[1])\n",
        "vf, tf = s[output].split(vf, factor=tile_f_factors[2])\n",
        "tf, fi = s[output].split(tf, factor=tile_f_factors[3])\n",
        "\n",
        "by, vy = s[output].split(y, factor=tile_y_factors[1])\n",
        "vy, ty = s[output].split(vy, factor=tile_y_factors[2])\n",
        "ty, yi = s[output].split(ty, factor=tile_y_factors[3])\n",
        "\n",
        "bx, vx = s[output].split(x, factor=tile_x_factors[1])\n",
        "vx, tx = s[output].split(vx, factor=tile_x_factors[2])\n",
        "tx, xi, = s[output].split(tx, factor=tile_x_factors[3])\n",
        "\n",
        "kernel_scope = n  # this is the scope to attach global config inside this kernel\n",
        "\n",
        "s[output].reorder(n, bf, by, bx, vf, vy, vx, tf, ty, tx, fi, yi, xi)\n",
        "print(tvm.lower(s, [input, kernel, conv], simple_mode=True))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "// attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "allocate pad_temp.shared[float32 * 41472]\n",
            "// attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "allocate pad_temp.shared.local[float32 * 41472]\n",
            "// attr [kernel.shared] storage_scope = \"shared\"\n",
            "allocate kernel.shared[float32 * 2359296]\n",
            "// attr [kernel.shared.local] storage_scope = \"local\"\n",
            "allocate kernel.shared.local[float32 * 2359296]\n",
            "// attr [compute.local] storage_scope = \"local\"\n",
            "allocate compute.local[float32 * 25088]\n",
            "produce pad_temp.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)] = tvm_if_then_else(((((1 <= ax2) && (ax2 < 8)) && (1 <= ax3)) && (ax3 < 8)), data[(((((ax1*7) + ax2)*7) + ax3) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce pad_temp.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared.local[((((ax1*9) + ax2)*9) + ax3)] = pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared.local {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared.local[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute.local {\n",
            "  for (ff.c, 0, 512) {\n",
            "    for (yy.c, 0, 7) {\n",
            "      for (xx.c, 0, 7) {\n",
            "        compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = 0.000000f\n",
            "        for (rc, 0, 512) {\n",
            "          for (ry, 0, 3) {\n",
            "            for (rx, 0, 3) {\n",
            "              compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = (compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] + (pad_temp.shared.local[((((((rc*9) + yy.c) + ry)*9) + xx.c) + rx)]*kernel.shared.local[((((((ff.c*512) + rc)*3) + ry)*3) + rx)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  for (ff.outer, 0, 64) {\n",
            "    for (ff.inner.inner.outer, 0, 8) {\n",
            "      for (yy.inner.inner.outer, 0, 7) {\n",
            "        for (xx.inner.inner.outer, 0, 7) {\n",
            "          compute[((((((ff.outer*8) + ff.inner.inner.outer)*7) + yy.inner.inner.outer)*7) + xx.inner.inner.outer)] = compute.local[((((((ff.outer*8) + ff.inner.inner.outer)*7) + yy.inner.inner.outer)*7) + xx.inner.inner.outer)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH6gor8XjvXM"
      },
      "source": [
        "After reshaping the loop nest, we can bind portions of the computations to GPU blocks and threads. Additionally, we bind some loops to \"virtual\" threads which are effectively threads emulated in software. Virtual threads enable the expression of more sophisticated computation and memory access patterns vs. blocks and threads alone. Note binding effictively removes the associated loop axes from the schedule, as they are now parallelized based on their index instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yIhMYiQjvhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f7bf2d5-ba73-441d-d0d4-86dc2bb3a9c6"
      },
      "source": [
        "s[output].bind(bf, tvm.thread_axis(\"blockIdx.z\"))\n",
        "s[output].bind(by, tvm.thread_axis(\"blockIdx.y\"))\n",
        "s[output].bind(bx, tvm.thread_axis(\"blockIdx.x\"))\n",
        "s[output].bind(vf, tvm.thread_axis(\"vthread\"))\n",
        "s[output].bind(vy, tvm.thread_axis(\"vthread\"))\n",
        "s[output].bind(vx, tvm.thread_axis(\"vthread\"))\n",
        "s[output].bind(tf, tvm.thread_axis(\"threadIdx.z\"))\n",
        "s[output].bind(ty, tvm.thread_axis(\"threadIdx.y\"))\n",
        "s[output].bind(tx, tvm.thread_axis(\"threadIdx.x\"))\n",
        "s[OL].compute_at(s[output], tx)\n",
        "print(tvm.lower(s, [input, kernel, output], simple_mode=True))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "// attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "allocate pad_temp.shared[float32 * 4608]\n",
            "// attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "allocate pad_temp.shared.local[float32 * 4608]\n",
            "// attr [kernel.shared.local] storage_scope = \"local\"\n",
            "allocate kernel.shared.local[float32 * 4608]\n",
            "produce pad_temp.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)] = tvm_if_then_else((((((1 - threadIdx.y) <= ax2) && (ax2 < (8 - threadIdx.y))) && ((1 - threadIdx.x) <= ax3)) && (ax3 < (8 - threadIdx.x))), data[(((((((ax1*7) + ax2) + threadIdx.y)*7) + ax3) + threadIdx.x) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce pad_temp.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        pad_temp.shared.local[((((ax1*3) + ax2)*3) + ax3)] = pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)] = kernel[((((((((blockIdx.z*8) + threadIdx.z)*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        kernel.shared.local[((((ax1*3) + ax2)*3) + ax3)] = pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  // attr [iter_var(blockIdx.z, , blockIdx.z)] thread_extent = 64\n",
            "  // attr [compute.local] storage_scope = \"local\"\n",
            "  allocate compute.local[float32 * 1]\n",
            "  // attr [iter_var(blockIdx.y, , blockIdx.y)] thread_extent = 1\n",
            "  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 1\n",
            "  // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 8\n",
            "  // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "  produce compute.local {\n",
            "    compute.local[0] = 0.000000f\n",
            "    for (rc, 0, 512) {\n",
            "      for (ry, 0, 3) {\n",
            "        for (rx, 0, 3) {\n",
            "          compute.local[0] = (compute.local[0] + (pad_temp.shared.local[((((rc*3) + ry)*3) + rx)]*kernel.shared.local[((((rc*3) + ry)*3) + rx)]))\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  compute[((((((blockIdx.z*8) + threadIdx.z)*7) + threadIdx.y)*7) + threadIdx.x)] = compute.local[0]\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSicMUUukiu2"
      },
      "source": [
        "Next, we apply a tiling transformation over the reduction axes, using a series of loop axes splits followed by a reorder as in the previous case. With this arrangement of loop axes, we also define the points at which each cached tensor is prepared to be read or written with `compute_at`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT992Qu6ki6n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d28836-b1d1-47c7-a18e-1c2da15a838a"
      },
      "source": [
        "# tile reduction axes\n",
        "n, f, y, x = s[OL].op.axis\n",
        "rc, ry, rx = s[OL].op.reduce_axis\n",
        "rc_factors = [512, 32, 1]\n",
        "rx_factors = [3, 3, 1]\n",
        "ry_factors = [3, 3, 1]\n",
        "rco, rcm = s[OL].split(rc, factor=rc_factors[1])\n",
        "rcm, rci = s[OL].split(rcm, factor=rc_factors[2])\n",
        "ryo, rym = s[OL].split(ry, factor=ry_factors[1])\n",
        "rym, ryi = s[OL].split(rym, factor=ry_factors[2])\n",
        "rxo, rxm = s[OL].split(rx, factor=rx_factors[1])\n",
        "rxm, rxi = s[OL].split(rxm, factor=rx_factors[2])\n",
        "s[OL].reorder(rco, ryo, rxo, rcm, rym, rxm, rci, ryi, rxi, n, f, y, x)\n",
        "\n",
        "s[AA].compute_at(s[OL], rxo)\n",
        "s[WW].compute_at(s[OL], rxo)\n",
        "s[AL].compute_at(s[OL], rxm)\n",
        "s[WL].compute_at(s[OL], rxm)\n",
        "\n",
        "print(tvm.lower(s, [input, kernel, output], simple_mode=True))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "produce compute {\n",
            "  // attr [iter_var(blockIdx.z, , blockIdx.z)] thread_extent = 64\n",
            "  // attr [compute.local] storage_scope = \"local\"\n",
            "  allocate compute.local[float32 * 1]\n",
            "  // attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "  allocate pad_temp.shared[float32 * 2592]\n",
            "  // attr [kernel.shared] storage_scope = \"shared\"\n",
            "  allocate kernel.shared[float32 * 2304]\n",
            "  // attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "  allocate pad_temp.shared.local[float32 * 1]\n",
            "  // attr [kernel.shared.local] storage_scope = \"local\"\n",
            "  allocate kernel.shared.local[float32 * 1]\n",
            "  // attr [iter_var(blockIdx.y, , blockIdx.y)] thread_extent = 1\n",
            "  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 1\n",
            "  // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 8\n",
            "  // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "  produce compute.local {\n",
            "    compute.local[0] = 0.000000f\n",
            "    for (rc.outer, 0, 16) {\n",
            "      produce pad_temp.shared {\n",
            "        for (ax1, 0, 32) {\n",
            "          for (ax2, 0, 9) {\n",
            "            for (ax3, 0, 9) {\n",
            "              pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)] = tvm_if_then_else(((((1 <= ax2) && (ax2 < 8)) && (1 <= ax3)) && (ax3 < 8)), data[(((((((rc.outer*32) + ax1)*7) + ax2)*7) + ax3) + -8)], 0.000000f)\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      produce kernel.shared {\n",
            "        for (ax0, 0, 8) {\n",
            "          for (ax1, 0, 32) {\n",
            "            for (ax2, 0, 3) {\n",
            "              for (ax3, 0, 3) {\n",
            "                kernel.shared[((((((ax0*32) + ax1)*3) + ax2)*3) + ax3)] = kernel[((((((((((blockIdx.z*8) + ax0)*16) + rc.outer)*32) + ax1)*3) + ax2)*3) + ax3)]\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      for (rc.inner.outer, 0, 32) {\n",
            "        for (ry.inner.outer, 0, 3) {\n",
            "          for (rx.inner.outer, 0, 3) {\n",
            "            produce pad_temp.shared.local {\n",
            "              pad_temp.shared.local[0] = pad_temp.shared[((((((rc.inner.outer*9) + threadIdx.y) + ry.inner.outer)*9) + threadIdx.x) + rx.inner.outer)]\n",
            "            }\n",
            "            produce kernel.shared.local {\n",
            "              kernel.shared.local[0] = kernel.shared[((((((threadIdx.z*32) + rc.inner.outer)*3) + ry.inner.outer)*3) + rx.inner.outer)]\n",
            "            }\n",
            "            compute.local[0] = (compute.local[0] + (pad_temp.shared.local[0]*kernel.shared.local[0]))\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  compute[((((((blockIdx.z*8) + threadIdx.z)*7) + threadIdx.y)*7) + threadIdx.x)] = compute.local[0]\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfBPrhkRmMKC"
      },
      "source": [
        "Next, we schedule the cooperative fetching of data and weighs for the threads in each thread block. We use the `bind` schedule primitive as before, with the main difference being that we must split the thread axes to match the number of threads that have already been declared in the prevous spatial tiling step.\n",
        "For simplicity, we omit the specification of compiler directives (e.g., loop unrolling) in this part of the tutorial---we include them when showing the full AutoTVM tuining example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdW3ZMB9mMVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a042ad9-4570-462a-cff2-a93a8b3949fb"
      },
      "source": [
        "# cooperative fetching\n",
        "for load in [AA, WW]:\n",
        "    n, f, y, x = s[load].op.axis \n",
        "    fused = s[load].fuse(n, f, y, x)\n",
        "    tz, fused = s[load].split(fused, nparts=tile_f_factors[2])\n",
        "    ty, fused = s[load].split(fused, nparts=tile_y_factors[2])\n",
        "    tx, fused = s[load].split(fused, nparts=tile_x_factors[2])\n",
        "    s[load].bind(tz, tvm.thread_axis(\"threadIdx.z\"))\n",
        "    s[load].bind(ty, tvm.thread_axis(\"threadIdx.y\"))\n",
        "    s[load].bind(tx, tvm.thread_axis(\"threadIdx.x\"))\n",
        "print(tvm.lower(s, [input, kernel, output], simple_mode=True))\n",
        "# tune unroll\n",
        "#s[output].pragma(kernel_scope, 'auto_unroll_max_step', cfg['auto_unroll_max_step'].val)\n",
        "#s[output].pragma(kernel_scope, 'unroll_explicit', cfg['unroll_explicit'].val)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "produce compute {\n",
            "  // attr [iter_var(blockIdx.z, , blockIdx.z)] thread_extent = 64\n",
            "  // attr [compute.local] storage_scope = \"local\"\n",
            "  allocate compute.local[float32 * 1]\n",
            "  // attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "  allocate pad_temp.shared[float32 * 2592]\n",
            "  // attr [kernel.shared] storage_scope = \"shared\"\n",
            "  allocate kernel.shared[float32 * 2304]\n",
            "  // attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "  allocate pad_temp.shared.local[float32 * 1]\n",
            "  // attr [kernel.shared.local] storage_scope = \"local\"\n",
            "  allocate kernel.shared.local[float32 * 1]\n",
            "  // attr [iter_var(blockIdx.y, , blockIdx.y)] thread_extent = 1\n",
            "  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 1\n",
            "  // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 8\n",
            "  // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "  produce compute.local {\n",
            "    compute.local[0] = 0.000000f\n",
            "    for (rc.outer, 0, 16) {\n",
            "      produce pad_temp.shared {\n",
            "        // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 8\n",
            "        // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "        // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "        for (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner, 0, 7) {\n",
            "          if (likely(((threadIdx.z*4) < (32 - ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/81))))) {\n",
            "            if (likely(((threadIdx.z*36) < (288 - ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/9))))) {\n",
            "              if (likely(((threadIdx.z*324) < (((2592 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) - (threadIdx.x*7)) - (threadIdx.y*47))))) {\n",
            "                if (likely(((threadIdx.y*47) < ((324 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) - (threadIdx.x*7))))) {\n",
            "                  if (likely(((threadIdx.x*7) < (47 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)))) {\n",
            "                    pad_temp.shared[((threadIdx.z*324) + (((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner))] = tvm_if_then_else(((((9 <= ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 81)) && (((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 81) < 72)) && (1 <= ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 9))) && (((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 9) < 8)), data[((((((((((((threadIdx.z*4) + ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/81))/32)*16) + rc.outer)*32) + (((threadIdx.z*4) + ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/81)) % 32))*7) + (((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 81)/9))*7) + ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 9)) + -8)], 0.000000f)\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      produce kernel.shared {\n",
            "        // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 8\n",
            "        // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "        // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "        for (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner, 0, 6) {\n",
            "          if (likely(((((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3))/96) < (8 - threadIdx.z)))) {\n",
            "            if (likely(((threadIdx.z*32) < (256 - (((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3))/3))))) {\n",
            "              if (likely(((threadIdx.z*96) < (((768 - (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3)) - (threadIdx.x*2)) - (threadIdx.y*14))))) {\n",
            "                if (likely(((threadIdx.z*288) < (((2304 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) - (threadIdx.x*6)) - (threadIdx.y*42))))) {\n",
            "                  if (likely(((((threadIdx.y*7) + threadIdx.x)*6) < (288 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)))) {\n",
            "                    if (likely(((blockIdx.z*8) < ((512 - threadIdx.z) - (((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3))/96))))) {\n",
            "                      kernel.shared[((((threadIdx.z*96) + ((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3)))*3) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner % 3))] = kernel[(((((((((blockIdx.z*8) + (((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3))/96)) + threadIdx.z)*16) + rc.outer)*96) + (((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3)) % 96))*3) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner % 3))]\n",
            "                    }\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      for (rc.inner.outer, 0, 32) {\n",
            "        for (ry.inner.outer, 0, 3) {\n",
            "          for (rx.inner.outer, 0, 3) {\n",
            "            produce pad_temp.shared.local {\n",
            "              pad_temp.shared.local[0] = pad_temp.shared[((((((rc.inner.outer*9) + threadIdx.y) + ry.inner.outer)*9) + threadIdx.x) + rx.inner.outer)]\n",
            "            }\n",
            "            produce kernel.shared.local {\n",
            "              kernel.shared.local[0] = kernel.shared[((((((threadIdx.z*32) + rc.inner.outer)*3) + ry.inner.outer)*3) + rx.inner.outer)]\n",
            "            }\n",
            "            compute.local[0] = (compute.local[0] + (pad_temp.shared.local[0]*kernel.shared.local[0]))\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  compute[((((((blockIdx.z*8) + threadIdx.z)*7) + threadIdx.y)*7) + threadIdx.x)] = compute.local[0]\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRZZDR5wrRA_"
      },
      "source": [
        "Finally, we run a reference implementation to generate reference results to check the correctness of the scheduled code and measure the performance of the scheduled code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmYLCxlbrRKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "929a409a-c062-49f6-ed04-961f9a465400"
      },
      "source": [
        "# check correctness\n",
        "a_np = np.random.uniform(size=(N, CI, H, W)).astype(np.float32)\n",
        "w_np = np.random.uniform(size=(CO, CI, KH, KW)).astype(np.float32)\n",
        "c_np = conv2d_nchw_python(a_np, w_np, stride, padding)\n",
        "\n",
        "# compile the manually schedule convolution\n",
        "with tvm.target.create('cuda'):\n",
        "    manual_conv2d = tvm.build(s, [input, kernel, output])\n",
        "    \n",
        "ctx = tvm.gpu()\n",
        "a_tvm = tvm.nd.array(a_np, ctx=ctx)\n",
        "w_tvm = tvm.nd.array(w_np, ctx=ctx)\n",
        "c_tvm = tvm.nd.empty(c_np.shape, ctx=ctx)\n",
        "manual_conv2d(a_tvm, w_tvm, c_tvm)\n",
        "\n",
        "tvm.testing.assert_allclose(c_np, c_tvm.asnumpy(), rtol=1e-2)\n",
        "\n",
        "evaluator = manual_conv2d.time_evaluator(manual_conv2d.entry_name, ctx, number=400)\n",
        "mean = evaluator(a_tvm, w_tvm, c_tvm).mean\n",
        "print(\"complexity: \", autotvm.task.task.compute_flop(s))\n",
        "print(\"Time cost of this operator: %f\" % mean)\n",
        "manual_flops = autotvm.task.task.compute_flop(s)/mean\n",
        "print(\"GFLOPS:\", manual_flops/1e9)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "complexity:  231501312\n",
            "Time cost of this operator: 0.001017\n",
            "GFLOPS: 227.5949809918468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R0J9j5m_CBY"
      },
      "source": [
        "Using a schedule template instead of a manually defined schedule\n",
        "====================================================\n",
        "\n",
        "Next, we show that we can avoid the magic numbers used previously and instead leave them as free variables to be decided by a tuner. This change relieves the burden of tuning on the schedule writer and also potentially opens up a much large space for optimization.\n",
        "\n",
        "From a high level schedule template is identical to the manual schedule defined previously, with hardcoded values replaced with configuration option declarations. Note that AutoTVM also provides some syntactic sugar for splitting (using `define_split`) a single axis into multiple axes at once with the `num_outputs` parameter, instead of only splitting each axis into two each time with `split`. Also note that each configuration option is now applied with calls to `cfg[...].apply(...)`.\n",
        "\n",
        "Note that we wrap the schedule in a function to leverage the `@autotvm.template` decorator for automated tuning.\n",
        "Tunable parameters that were previously manually specified are now passed using the `cfg` variable in the schedule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sgFW_oi_OHr"
      },
      "source": [
        "@autotvm.template\n",
        "def conv2d_no_batching(N, H, W, CO, CI, KH, KW, stride, padding):\n",
        "    assert N == 1, \"Only consider batch_size = 1 in this template\"\n",
        "\n",
        "    data = tvm.placeholder((N, CI, H, W), name='data')\n",
        "    kernel = tvm.placeholder((CO, CI, KH, KW), name='kernel')\n",
        "    conv = topi.nn.conv2d_nchw(data, kernel, stride, padding, dilation=1, out_dtype='float32')\n",
        "    s = tvm.create_schedule([conv.op])\n",
        "\n",
        "    ##### space definition begin #####\n",
        "    n, f, y, x = s[conv].op.axis\n",
        "    rc, ry, rx = s[conv].op.reduce_axis\n",
        "\n",
        "    cfg = autotvm.get_config()\n",
        "    cfg.define_split(\"tile_f\", f, num_outputs=4)\n",
        "    cfg.define_split(\"tile_y\", y, num_outputs=4)\n",
        "    cfg.define_split(\"tile_x\", x, num_outputs=4)\n",
        "    cfg.define_split(\"tile_rc\", rc, num_outputs=3)\n",
        "    cfg.define_split(\"tile_ry\", ry, num_outputs=3)\n",
        "    cfg.define_split(\"tile_rx\", rx, num_outputs=3)\n",
        "    cfg.define_knob(\"auto_unroll_max_step\", [0, 512, 1500])\n",
        "    cfg.define_knob(\"unroll_explicit\", [0, 1])\n",
        "    ##### space definition end #####\n",
        "\n",
        "    # inline padding\n",
        "    pad_data = s[conv].op.input_tensors[0]\n",
        "    s[pad_data].compute_inline()\n",
        "    data, raw_data = pad_data, data\n",
        "\n",
        "    output = conv\n",
        "    OL = s.cache_write(conv, 'local')\n",
        "\n",
        "    # create cache stage\n",
        "    AA = s.cache_read(data, 'shared', [OL])\n",
        "    WW = s.cache_read(kernel, 'shared', [OL])\n",
        "    AL = s.cache_read(AA, 'local', [OL])\n",
        "    WL = s.cache_read(WW, 'local', [OL])\n",
        "\n",
        "    # tile and bind spatial axes\n",
        "    n, f, y, x = s[output].op.axis\n",
        "    bf, vf, tf, fi = cfg[\"tile_f\"].apply(s, output, f)\n",
        "    by, vy, ty, yi = cfg[\"tile_y\"].apply(s, output, y)\n",
        "    bx, vx, tx, xi = cfg[\"tile_x\"].apply(s, output, x)\n",
        "    kernel_scope = n  # this is the scope to attach global config inside this kernel\n",
        "\n",
        "    s[output].bind(bf, tvm.thread_axis(\"blockIdx.z\"))\n",
        "    s[output].bind(by, tvm.thread_axis(\"blockIdx.y\"))\n",
        "    s[output].bind(bx, tvm.thread_axis(\"blockIdx.x\"))\n",
        "    s[output].bind(vf, tvm.thread_axis(\"vthread\"))\n",
        "    s[output].bind(vy, tvm.thread_axis(\"vthread\"))\n",
        "    s[output].bind(vx, tvm.thread_axis(\"vthread\"))\n",
        "    s[output].bind(tf, tvm.thread_axis(\"threadIdx.z\"))\n",
        "    s[output].bind(ty, tvm.thread_axis(\"threadIdx.y\"))\n",
        "    s[output].bind(tx, tvm.thread_axis(\"threadIdx.x\"))\n",
        "    s[output].reorder(n, bf, by, bx, vf, vy, vx, tf, ty, tx, fi, yi, xi)\n",
        "    s[OL].compute_at(s[output], tx)\n",
        "\n",
        "    # tile reduction axes\n",
        "    n, f, y, x = s[OL].op.axis\n",
        "    rc, ry, rx = s[OL].op.reduce_axis\n",
        "    rco, rcm, rci = cfg['tile_rc'].apply(s, OL, rc)\n",
        "    ryo, rym, ryi = cfg['tile_rx'].apply(s, OL, ry)\n",
        "    rxo, rxm, rxi = cfg['tile_ry'].apply(s, OL, rx)\n",
        "    s[OL].reorder(rco, ryo, rxo, rcm, rym, rxm, rci, ryi, rxi, n, f, y, x)\n",
        "\n",
        "    s[AA].compute_at(s[OL], rxo)\n",
        "    s[WW].compute_at(s[OL], rxo)\n",
        "    s[AL].compute_at(s[OL], rxm)\n",
        "    s[WL].compute_at(s[OL], rxm)\n",
        "\n",
        "    # cooperative fetching\n",
        "    for load in [AA, WW]:\n",
        "        n, f, y, x = s[load].op.axis\n",
        "        fused = s[load].fuse(n, f, y, x)\n",
        "        tz, fused = s[load].split(fused, nparts=cfg[\"tile_f\"].size[2])\n",
        "        ty, fused = s[load].split(fused, nparts=cfg[\"tile_y\"].size[2])\n",
        "        tx, fused = s[load].split(fused, nparts=cfg[\"tile_x\"].size[2])\n",
        "        s[load].bind(tz, tvm.thread_axis(\"threadIdx.z\"))\n",
        "        s[load].bind(ty, tvm.thread_axis(\"threadIdx.y\"))\n",
        "        s[load].bind(tx, tvm.thread_axis(\"threadIdx.x\"))\n",
        "\n",
        "    # tune unroll\n",
        "    s[output].pragma(kernel_scope, 'auto_unroll_max_step', cfg['auto_unroll_max_step'].val)\n",
        "    s[output].pragma(kernel_scope, 'unroll_explicit', cfg['unroll_explicit'].val)\n",
        "\n",
        "    return s, [raw_data, kernel, conv]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-CFfMD6Ie1O"
      },
      "source": [
        "Start Infrastructure for Tuning (tracker)\n",
        "===============================\n",
        "AutoTVM leverages the TVM RPC system to abstract and multiplex the actual hardware tuning targets. TVM RPC provides a tracker that distributes hardware resources so that multiple tuning jobs can share a pool of hardware devices. In this case we start a tracker instance on the machine running the laboratory notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ch6kPVaIfDo"
      },
      "source": [
        "%%script bash --bg --out output --err error\n",
        "PYTHONPATH=/tvm/python:$PYTHONPATH && python3 -m tvm.exec.rpc_tracker --host 0.0.0.0 --port 9190 &\n",
        "while true; do\n",
        "  res=$(PYTHONPATH=/tvm/python:$PYTHONPATH && python3 -m tvm.exec.query_rpc_tracker --host 0.0.0.0 --port 9190 2>&1 | grep 'Cannot connect to tracker')\n",
        "  if [ \"$res\" == \"\" ]; then\n",
        "    echo \"OK @ \" $(date) \"...\" >> status.log\n",
        "  else\n",
        "    echo \"RESTARTING @ \" $(date) \"...\" >> status.log\n",
        "    PYTHONPATH=/tvm/python:$PYTHONPATH && python3 -m tvm.exec.rpc_tracker --host 0.0.0.0 --port 9190 &\n",
        "  fi\n",
        "  sleep 5\n",
        "done"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8v4NcPtIm8R"
      },
      "source": [
        "Start Infrastructure for Tuning (server)\n",
        "===============================\n",
        "We then start an RPC server instance that manages this notebook's GPU, and configure it to report to the tracker."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liZMOINaIm_v"
      },
      "source": [
        "%%script bash --bg --out output2 --err error2\n",
        "while true; do\n",
        "echo \"started server at \" $(date) >> status.log\n",
        "PYTHONPATH=/tvm/python:/tvm/topi/python:$PYTHONPATH && python3 -m tvm.exec.rpc_server --key 1080ti --tracker 0.0.0.0:9190\n",
        "sleep 30\n",
        "done"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRzivXLpKtCk"
      },
      "source": [
        "Check the status of the tracker\n",
        "========================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6R8SoScKtJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11731306-d391-4721-866c-94c89911d1c0"
      },
      "source": [
        "! cat status.log | tail\n",
        "! PYTHONPATH=/tvm/python:$PYTHONPATH && python3 -m tvm.exec.query_rpc_tracker --host 0.0.0.0 --port 9190 "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "started server at  Fri Sep 23 08:22:07 UTC 2022\n",
            "Tracker address 0.0.0.0:9190\n",
            "\n",
            "Server List\n",
            "----------------------------\n",
            "server-address\tkey\n",
            "----------------------------\n",
            "127.0.0.1:44002\tserver:1080ti\n",
            "----------------------------\n",
            "\n",
            "Queue Status\n",
            "------------------------------\n",
            "key      total  free  pending\n",
            "------------------------------\n",
            "1080ti   1      1     0      \n",
            "------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC6QtSgnw29Q"
      },
      "source": [
        "peak = 0.0\n",
        "count = 0\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def plot_callback():\n",
        "  y = list()\n",
        "  x = list()\n",
        "  fig,ax = plt.subplots(1,1)\n",
        "  def _callback(_, inputs, results):\n",
        "    global peak\n",
        "    global count\n",
        "    for inp, res in zip(inputs, results):\n",
        "      count += 1\n",
        "      if res.error_no == 0:\n",
        "        cost = np.mean(res.costs)\n",
        "        perf = inp.task.flop/cost\n",
        "        if perf > peak:\n",
        "          print(\"reached new peak: {:.2f} GFLOPS\".format(perf/1e9))\n",
        "          peak = perf\n",
        "      x.append(count)\n",
        "      y.append(peak)\n",
        "      if count % 8 == 0:\n",
        "        plt.plot(x, y)\n",
        "        plt.axhline(y=manual_flops, color='r', linestyle=':', label='manual baseline')\n",
        "        plt.ylabel('performance (FLOP/S)')\n",
        "        plt.xlabel('trials run')\n",
        "        plt.legend()\n",
        "        clear_output()\n",
        "        plt.show()\n",
        "  return _callback"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfznWuPiIVQw"
      },
      "source": [
        "Launch the Tuning Process\n",
        "========================\n",
        "We first acquire a logger to show the results of tuning and define the tuning task to be the last convolutional layer of resnet-18.\n",
        "The remaining code is boilerplate for specifying tuning options, specifying that we are running tuning over RPC and the target RPC device type and timeouts for building and running schedule configurations. Finally, we launch the tuning job with `tuner.tune()`, passing the number of trials (number of configurations to profile) that we want to allocate for tuning.\n",
        "\n",
        "\n",
        "Note that due to the balance of CPU and GPU resources on colab notebook runtimes, we use XGB `knob` features, which are much less CPU intensive to compute than `itervar` features. However, `itervar` features remain useful when we want to leverage transfer learning for efficient tuning across multiple tuning jobs.\n",
        "\n",
        "Finally, we recommend using dedicated hardware resources for full-scale tuning experiments, as the stability of colab notebook runtimes and tuning is uncertain over long time periods in our experience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPgLaW8eIYdR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "7416757d-d1d4-459b-e598-1f6b05cd3651"
      },
      "source": [
        "# logging config (for printing tuning log to screen)\n",
        "#logging.getLogger('autotvm').setLevel(logging.DEBUG)\n",
        "#logging.getLogger('autotvm').addHandler(logging.StreamHandler(sys.stdout))\n",
        "\n",
        "# the last layer in resnet\n",
        "N, H, W, CO, CI, KH, KW, strides, padding = 1, 7, 7, 512, 512, 3, 3, (1, 1), (1, 1)\n",
        "task = autotvm.task.create(conv2d_no_batching,\n",
        "                           args=(N, H, W, CO, CI, KH, KW, strides, padding),\n",
        "                           target='cuda')\n",
        "\n",
        "# Use local gpu, measure 10 times for every config to reduce variance\n",
        "# The timeout of compiling a program is 10 seconds, the timeout for running is 4 seconds\n",
        "measure_option = autotvm.measure_option(\n",
        "    builder=autotvm.LocalBuilder(),\n",
        "            runner=autotvm.RPCRunner(\n",
        "            '1080ti',  # change the device key to your key\n",
        "            '0.0.0.0', 9190,\n",
        "            number=256, repeat=3, timeout=1, min_repeat_ms=50)\n",
        ")\n",
        "\n",
        "# Begin tuning, log records to file `conv2d.log`\n",
        "# During tuning we will also try many invalid configs, so you are expected to\n",
        "# see many error reports. As long as you can see non-zero GFLOPS, it is okay.\n",
        "tuner = autotvm.tuner.XGBTuner(task, feature_type='knob')\n",
        "tuner.tune(n_trial=512,\n",
        "           measure_option=measure_option,\n",
        "           callbacks=[plot_callback()])\n",
        "           #callbacks=[autotvm.callback.log_to_file('conv2d.log'), plot_callback()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lpizg_uWj0Qw"
      },
      "source": [
        "Conclusion\n",
        "-------------------\n",
        "We showed how to lift a schedule for an operator (direct convolution on GPU) to a schedule template, which defines a search spaces of possible implementations using the direct strategy on a GPU. We then tuned the operator to achieve better performance than a manually written schedule with hardcoded parameters. Crucially, tuning is automatic across many different operator shapes and and variants."
      ]
    }
  ]
}